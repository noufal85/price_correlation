Clustering Stocks by Price Correlation (Last 18 Months)OverviewThe goal is to identify groups of stocks (across the entire NASDAQ and NYSE universe) that move together – in other words, whose daily price changes are highly correlated. By analyzing the last 18 months of daily price data for thousands of stocks, we can apply machine learning clustering to find clusters of co-moving stocks. We will use a purely price-based approach, focusing on historical price movements (no fundamental or other data), and leverage a robust method grounded in academic research.Why cluster stocks by correlation? Stocks that are strongly correlated over time often share common factors – for example, they might belong to the same industry or sector, reacting similarly to economic news. Clustering can reveal this hidden structure. We want an algorithm that can automatically group such stocks without pre-defining categories like sectors. Our end result will be a set of clusters (groups of tickers) which we can export to a JSON file for further use (e.g. loading into a database).Data Collection and Preparation1. Collecting Price Data: We need daily price history for all stocks on NYSE and NASDAQ over the past 18 months. This is a large universe (on the order of thousands of stocks), so data gathering is a non-trivial step. In practice, one could use financial data APIs or libraries (e.g. Yahoo Finance via yfinance in Python, or an institutional data source) to retrieve adjusted daily closing prices for each ticker in the exchange listings. Adjusted prices are recommended to account for stock splits or dividends over the period. Ensure the date range is consistent – for example, if we consider 18 months up to January 2026, gather data from roughly July 2024 through December 2025 (approximately 390-400 trading days, since weekends/holidays have no trading).2. Filtering and Alignment: Some stocks might not have data for the full period (e.g. newly listed IPOs) – you may exclude those or handle missing data as needed. It’s important to align all time series by date. We can create a common timeline of trading days and ensure each stock’s prices are aligned to that. If a stock didn’t trade (missing data) on a particular day (perhaps due to suspension or corporate actions), one approach is to forward-fill the last known price or simply treat the return as 0% for that day – but ideally, it’s better to remove any day with widespread missing data or exclude stocks with too many missing points. For simplicity, we assume we have a complete aligned dataset of daily prices P_(i,t) for stock i on day t, for t = 1...T (T ~ 390 days, i = 1...N stocks).3. Calculate Daily Returns: Convert the price series into daily returns, which measure the price change from one day to the next. This normalizes the data and focuses on relative movements. A common choice is log returns:R_(i,t)=ln(P_(i,t) )-ln(P_(i,t-1) )Alternatively, simple percentage returns can be used:R_(i,t)=(P_(i,t)-P_(i,t-1))/P_(i,t-1) .Using returns (especially log-returns) is standard in finance because it makes the data more stationary and ensures that we compare proportional changes. Compute these daily returns for each stock over the 18-month window. We will end up with an N ? T matrix of returns (N stocks, T days).4. Construct the Return Matrix: Organize the data in a matrix or DataFrame where each row corresponds to a stock and each column to a specific trading date (or vice versa). For example, each row i could be the return series R_(i,1...T) for stock i. We’ll use this to compute correlations. If using Python with pandas, one might create a DataFrame indexed by date with columns as tickers, and use df.pct_change() or df.diff() to get daily returns easily.Computing the Correlation Matrix5. Compute Pairwise Correlations: Using the matrix of returns, compute the Pearson correlation coefficient between every pair of stock return series over the 18-month period. The Pearson correlation ?_ij between stock i and j captures how similarly their daily returns move together (values range from +1 for perfectly correlated, 0 for uncorrelated, down to –1 for perfectly inversely correlated). This results in an N ? N correlation matrix C, where entry C_ij=?_ij. By definition C_ii=1 (each stock perfectly correlates with itself) and C_ij=C_ji.This correlation matrix is the core input to our clustering. It is essentially a similarity matrix: stocks with higher correlations are “closer” or more similar in terms of market behavior. In practice, with N in the thousands, the matrix will have millions of entries. However, storing this in memory (e.g. 5000?5000 ~ 25e6 entries) is feasible on modern hardware, and computing it can be done efficiently using vectorized operations or linear algebra routines.6. Interpreting Correlations: We expect many stocks to show some positive correlation due to overall market and sector effects. For example, large tech stocks might all have high mutual correlations because they respond to similar industry news. Banking stocks might form another highly correlated group. Stocks from unrelated sectors (say a utility company vs. a tech company) might show low correlation. By examining the raw correlation matrix, we might already see block patterns where certain groups of stocks all inter-correlate strongly (an indication of clusters). However, the matrix will also contain a lot of noise – especially since we have a limited time window (18 months) to estimate potentially thousands of relationships. Academic studies often employ techniques like random matrix theory to distinguish “real” correlations from random noise[1], but for this clustering project we will proceed with the raw correlations or a simple filtered approach, as the focus is on a straightforward, widely accepted method.Choosing the Clustering AlgorithmGiven the user’s request for the “most reliable and best (method) used and approved by academic research”, we will use hierarchical clustering on the correlation matrix. Hierarchical clustering has been a go-to method in academic finance for uncovering market structure from correlation data ever since the pioneering work of Mantegna (1999)[2]. It is unsupervised and does not require us to pre-specify the number of clusters, which is an advantage over simpler methods like k-means (which would require guessing how many clusters, k, to use)[3]. Hierarchical clustering produces a dendrogram (a tree of clusters) that can be cut at various levels to obtain different groupings, allowing flexible exploration of the data’s structure[3]. This is ideal for financial data where there may be a natural hierarchy (e.g. stocks clustering by industry, which further cluster by sector, etc.).Academic Background: Clustering stocks by price correlation was originally proposed by Mantegna (1999)[2], who showed that the approach yields a “meaningful economic taxonomy” of stocks. In that study and subsequent research, stocks that cluster together purely based on correlation often belong to the same industry or sector, reflecting underlying common factors in those groups[4]. For example, one can often see a bank cluster, a technology cluster, a healthcare cluster, and so on, emerging from the data without having told the algorithm anything about those companies’ business – the price co-movements alone reveal this structure. This gives confidence that hierarchical clustering is a reliable method here, as it aligns with real economic groupings and has been vetted in literature.Distance Metric for Correlation: Clustering algorithms operate on a notion of distance (dissimilarity) rather than similarity. Since correlation ?_ij is a similarity measure (higher means more similar), we convert it to a distance. A simple choice is “correlation distance” = 1 – ?_ij. However, to ensure the distance adheres to mathematical requirements (triangle inequality, etc.), a common transformation used in literature is:d_ij=√(?2?(1-?_ij )?)This is derived from correlation and is a proper metric in an ultrametric space, as discussed by Mantegna[5]. If two stocks have perfect correlation +1, d_ij=0 (zero distance). If they have correlation 0, d_ij=√2≈1.414. If correlation is –1 (perfect opposite movement), d_ij=√(2(1-(-1)) )=√4=2, which is the maximum distance in this scale. We will use this distance measure for clustering (in practice, using 1-? or the full sqrt formula yields the same cluster ordering since sqrt is monotonic; many implementations use 1-? for simplicity).Hierarchical Clustering Method: We will perform agglomerative hierarchical clustering, which starts with each stock as its own cluster and then merges clusters step by step. The key decision here is the linkage criterion – how to measure distance between clusters as they are formed. Common linkage methods are: single linkage (distance between two clusters = smallest pairwise distance between any member of one and any member of the other), complete linkage (distance = largest pairwise distance between a member of one and a member of the other), average linkage (distance = average of all pairwise distances between members of the two clusters), and Ward’s method (which minimizes the variance increase when merging clusters)[6].* Single linkage is closely related to the minimum spanning tree (MST) of the graph of stocks[7]. In fact, Mantegna’s original work essentially built the MST from the correlation distances to visualize the hierarchical structure. Single linkage tends to create a chaining effect, where clusters can be strung out like a chain – this can sometimes yield one giant cluster with a long tail of elements being added one by one[8]. While it preserves the true nearest-neighbor structure (good for creating an MST), it may not produce the most intuitive partitions for trading purposes (small clusters vs one big cluster issue).* Complete linkage tends to form tighter, more compact clusters (it won’t merge two groups unless all pairs are relatively close). This usually yields more balanced cluster sizes as compared to single linkage[8].* Average linkage is a compromise; it often produces results similar to Ward’s method and can handle noisy distances well[8]. Ward’s method is popular in finance for clustering correlation matrices because it focuses on variance, but it assumes Euclidean distances. Using our correlation distance (which is Euclidean in an abstract space thanks to the sqrt transform) makes Ward’s applicable too.Given academic guidance, we will likely choose average or complete linkage for robust clusters. Research indicates that these methods avoid the excessive chaining of single linkage and yield clusters that make sense (e.g., Raffinot (2018) notes that complete linkage gives more balanced asset clusters while single linkage can lead to one large chain cluster)[8]. In practical terms, average linkage (also known as UPGMA) is a solid choice when we don't have a strong reason to prefer smallest or largest distance. It will merge stocks into clusters based on the average correlation distance, which tends to group stocks that are all mutually close on average.No Pre-set Number of Clusters: A benefit of hierarchical clustering is we do not need to decide upfront how many clusters we will have[3]. The output is a dendrogram that shows clusters at every possible level of granularity (from all stocks in one cluster down to each stock in its own cluster). We can decide the optimal or desired number of clusters by “cutting” this dendrogram at a certain distance threshold or number of clusters. This flexibility is useful because the "right" number of clusters in financial data is not obvious – there could be a few broad clusters (e.g., perhaps grouping by sector) and those might further split into sub-clusters (industry groups). We will discuss how to choose the cut in the next section.Clustering Procedure Step-by-StepUsing the data and method described, here are the concrete steps to perform the clustering:1. Prepare Data Matrix: As described, ensure we have an N?T matrix of returns. In code, this could be a pandas DataFrame of shape (T, N) where each column is a stock’s return series, or (N, T) in a NumPy array. The key is we can compute correlations from it.2. Compute Correlation Matrix: Use a vectorized approach to compute the correlation matrix. For example, in Python:import numpy as np# returns_matrix is shape (N, T)corr_matrix = np.corrcoef(returns_matrix)  # shape (N, N)Most math/statistics environments have a function for correlation matrix. In pandas, df.corr() would also yield the correlation matrix between columns (stocks). This is the heavy computation step but can be done in optimized C/Fortran under the hood, so it’s quite fast even for large N, as long as memory can hold the result.3. Convert to Distance Matrix: Transform the correlation matrix into a distance matrix. We will use d_ij=1-?_ij (which is in [0, 2] range) or the sqrt version. Many clustering libraries accept a condensed distance matrix (an array of the upper/lower triangular distances). For instance, SciPy’s hierarchy.linkage expects a condensed distance array. We can do:from scipy.spatial.distance import squareform, pdist# Use pdist with 'correlation' metric which effectively does 1 - Pearson correlationdist_array = pdist(returns_matrix, metric='correlation')SciPy’s "correlation" metric for pdist computes 1- Pearson correlation between each pair of rows in the given matrix. (Here we need to ensure returns_matrix is shaped as (N, T) – each row a stock’s returns – for pdist to treat rows as observations.) The result dist_array is a one-dimensional array of length N*(N-1)/2 containing the pairwise distances. If using another environment, one could loop to fill an N?N distance matrix, but that’s inefficient for large N. Vectorized pdist or similar is preferable.4. Perform Hierarchical Clustering: With the distance data ready, apply the clustering algorithm. Using SciPy as an example:from scipy.cluster import hierarchyZ = hierarchy.linkage(dist_array, method='average')Here, method='average' specifies average linkage. You could use 'complete' or 'ward' as well (Ward would technically operate on squared distances or require raw data, but given our distance is Euclidean in a high-dimensional sense, Ward should be okay too). The result Z is the linkage matrix of shape (N-1, 4), which encodes the hierarchical clustering (each row of Z corresponds to a merger of two clusters at a certain distance). This is typically used to plot dendrograms or to derive cluster assignments.5. Determine the Number of Clusters: We now need to decide how many clusters to extract. There are a few approaches:* Distance Threshold: We can cut the dendrogram at a chosen distance. For example, we might decide that any pair of stocks with correlation < 0.3 (which is distance = 0.7) are too dissimilar to be in the same cluster, which would determine a cutoff. This is somewhat arbitrary, but we might fine-tune by looking at the dendrogram or distribution of distances in Z. Often, one looks for a large jump in the linkage distances – the idea is, if the dendrogram shows that merging beyond a certain point requires a big jump in distance, that jump indicates distinct clusters. We could inspect the distances in Z (the last column of Z is the distance at which the merge happens) and find an elbow or gap.* Fixed Number of Clusters (k): Alternatively, we might decide we want, say, 10 clusters (perhaps guessing that they might correspond to major sectors like Tech, Health, Finance, Energy, etc.). We can then cut the dendrogram to yield exactly k clusters. In SciPy, one can use hierarchy.fcluster(Z, k, criterion='maxclust') to directly obtain k clusters. This may or may not yield the most sensible grouping if k is not well-chosen, but it is a straightforward way if we know roughly how many clusters we want.* Statistical Criteria: There are academic methods like the gap statistic or the silhouette score to evaluate the number of clusters. The gap statistic compares the clustering result to a null reference (data with no inherent clusters)[9], and silhouette scores measure how separated the clusters are. These can be computationally intensive for large N, but they provide guidance. Another angle, particularly for correlation matrices, is to use eigenvalue analysis: Random Matrix Theory (Plerou et al. 2000s) suggests that the number of significant factors (eigenvalues beyond the bulk random distribution) in a correlation matrix could hint at how many meaningful clusters or factors there are[1]. For example, one often finds one very large eigenvalue (the market mode affecting all stocks) and then perhaps 10-15 medium eigenvalues corresponding to sector groupings. This might suggest on the order of a dozen clusters.For our purposes, a pragmatic approach is to examine the dendrogram and use domain knowledge. We might find, for example, a natural split into ~10–20 clusters that correspond to known sectors/industries. In a known example, clustering the 30 stocks of the Dow or an index often yields groups like tech stocks together, banks together, consumer goods together, energy together, etc. In an academic study on the Stockholm market, cutting a hierarchical tree yielded clusters clearly identified as Industrial, Bank, Security, Telecom, and Pulp & Paper companies[4]. We expect a similar outcome for the US market – e.g., a cluster for Financials (banks, insurers), Technology, Healthcare, Energy, Utilities, Consumer Discretionary, Consumer Staples, etc., roughly mirroring industry sectors. We will choose a number that seems to capture this granularity without over-clustering every tiny niche. Let’s say, for illustration, we aim for around 10 clusters to start (we can always adjust if needed).6. Assign Cluster Labels: Once we decide on the cutoff (either by distance or number of clusters), we assign each stock to a cluster. Using the SciPy example, if we wanted k clusters:cluster_labels = hierarchy.fcluster(Z, t=k, criterion='maxclust')This returns an array of length N with cluster indices (1 through k) for each stock. If using a distance threshold, one could do hierarchy.fcluster(Z, t=distance_threshold, criterion='distance'). Each stock now has a cluster ID.7. Inspect the Clusters (Validation): It’s wise to do a quick sanity check on the clusters. For instance, if we find one cluster contains Apple, Microsoft, Google, Amazon, etc., that makes sense as a tech/growth cluster. Another cluster might contain JPMorgan, Bank of America, Wells Fargo – a banking cluster. If we notice a cluster with seemingly unrelated companies, we might reconsider our cutoff or note that those stocks had some correlation quirk (sometimes, conglomerates or multi-sector companies can connect clusters). In academic analysis, hierarchical clustering indeed tends to align with industry classifications[4], confirming that price correlation is capturing fundamental economic commonalities.Exporting Results to JSONThe final step is to take the cluster assignments and export them in the desired format (JSON). We have flexibility in how to structure this, but given the use-case of loading into a database, a list of clusters is intuitive. For each cluster, we’ll list the member tickers:[  {    "cluster": 1,    "members": ["AAPL", "MSFT", "GOOGL", "AMZN", ...]    },  {    "cluster": 2,    "members": ["JPM", "BAC", "WFC", "C", ...]    },  {    "cluster": 3,    "members": ["XOM", "CVX", "COP", "SLB", ...]    },  ...]Each object in the list represents one cluster, with an identifier and the list of stock symbols in it. The cluster numbering (1, 2, 3, ...) is arbitrary – it might correspond to the order in which clusters were formed or just a sequential label. We could also use more descriptive names (like "Tech Cluster", "Banking Cluster") if we manually identify them, but since this is an automated process, numeric labels are fine. The JSON structure above is easy to parse and can be inserted into a document database or processed in code.To generate this JSON, we can do something like (continuing the Python pseudo-code):import jsonclusters = {}for stock, label in zip(list_of_stocks, cluster_labels):    clusters.setdefault(label, []).append(stock)# Convert to list of dictscluster_list = [{"cluster": int(lbl), "members": tickers} for lbl, tickers in clusters.items()]# Save to filewith open("stock_clusters.json", "w") as f:    json.dump(cluster_list, f, indent=2)This will group stocks by their cluster label and output the JSON. The indent=2 is just for readability.Note: If the number of stocks is very large, the JSON file will also be large. Ensure your database or downstream application can handle ingesting it. JSON is human-readable but not the most compact; however, it’s fine for a few thousand entries. Another format could be CSV (with two columns: stock, cluster_id) which might be easier for relational databases, but since JSON was requested, we stick to that.Further Considerations and Enhancements* Cluster Interpretability: After obtaining clusters, it can be insightful to see what they represent. Often, clusters correspond to sectors (as noted, e.g., a cluster of banks, a cluster of oil companies, etc.). This can be verified by cross-referencing the cluster members with industry classification. The result being purely price-based is important – it means we discovered these groupings without using any industry labels or fundamental data. It’s a strong validation of market efficiency and the idea that stocks in the same sector tend to move together due to common factors[4].* Stability and “Best” Method: Hierarchical clustering (with appropriate linkage) is a proven method in literature for this task[2]. It is deterministic (no random initialization as in k-means) and yields a full hierarchy of clusters. Academic research has continued to refine clustering approaches. For instance, there are spectral clustering methods and block-model clustering techniques specifically designed for financial correlation matrices. Recent work by Tang et al. (2021) introduced a “correlation blockmodel clustering” algorithm that assumes stocks in the same cluster have the same correlation pattern with all other stocks[10]. This approach can optimally select a small set of representative stocks from each cluster to mimic the diversity of the whole market[10]. Similarly, spectral clustering methods like the “Blockbuster” algorithm (Brownlees et al.) and “SPONGE” (Cucuringu et al.) use graph theory (eigen-decompositions of a Laplacian matrix derived from correlations) to find clusters of assets[11]. These methods are at the frontier of research and can handle some challenges (like noisy correlations or optimal number of clusters) in elegant ways. However, they are more complex to implement from scratch. For a project where we want a robust, well-understood solution, hierarchical clustering remains an excellent choice – it has stood the test of time in academic studies and is relatively straightforward to execute.* Choosing Linkage and Distance – impact: If we had chosen a different approach (say k-means clustering on the raw return time series or on principal components), the results could differ. k-means would require deciding k in advance and uses a different notion of distance (Euclidean distance on returns, which isn’t directly aligned with correlation). Hierarchical clustering with correlation distance focuses exactly on our key measure (co-movement similarity) and does not need k upfront, which is a big plus when we “don’t know” the structure beforehand[3]. The use of average linkage (or complete linkage) ensures we don’t end up with trivial or chain-like clusters, improving reliability of the result[8]. In summary, this method is both academically endorsed and practically effective for discovering groups of correlated stocks.* Updating and Maintenance: The clusters we find are based on the last 18 months. Markets evolve, so these relationships can change. It might be useful to recompute the clusters periodically (e.g., monthly or quarterly) using a rolling window of 18 months. By comparing cluster compositions over time, one could see if certain stocks migrate between clusters (perhaps a company changed its business model or a sector’s dynamics changed). The procedure would remain the same for each update.* Using the Cluster Results: Once the JSON of clusters is loaded into a database, you can use it in various ways. For example:* Diversification: From each cluster, pick one stock for a portfolio to ensure you’re not picking two highly redundant stocks (since within a cluster they all move together)[10].* Long-short strategies: Identify pairs or groups within a cluster that deviate from each other and bet on convergence (because if they usually move together, a divergence might be an opportunity).* Risk management: Understand that if you hold many stocks from the same cluster, your portfolio is less diversified than it appears, since those stocks could all drop or rise together on some news.* Sector rotation: If clusters correspond to sectors, one could rotate investments by favoring clusters that are in an uptrend and avoiding clusters in a downtrend, without explicitly using sector labels.In conclusion, by following this plan, we use a data-driven, academically grounded approach to cluster stocks by price movements. Hierarchical clustering on a correlation matrix is a well-established technique that will reveal groups of stocks moving in unison[2]. The output, formatted in JSON, will list those clusters for easy consumption. This method ensures we capture the intrinsic market structure (e.g., industry groupings) purely from price data, meeting the project’s requirements and leveraging the best practices identified in financial literature.Sources:* Mantegna, R.N. (1999). Hierarchical structure in financial markets. European Physical Journal B 11, 193–197. – (Introduced the concept of clustering stocks by price correlation and using a hierarchical tree to reveal economic groupings)[2][5].* Tumminello et al. (2007). Correlation based networks in financial markets. – (Discusses constructing minimum spanning trees and hierarchical clustering from correlation matrices, reinforcing that such clusters often correspond to sectors).* Sandoval, L. & Franca, I. (2012). Correlation of financial markets in times of crisis. – (Applied hierarchical clustering to global markets, illustrating how co-movement structures change in crises).* Östlund, R. (2009). Correlation based clustering of the Stockholm Stock Exchange (Master’s Thesis) – (Demonstrates hierarchical clustering on Stockholm stocks; shows clear clusters by industry using average linkage)[4].* Tang, W. et al. (2021). Asset Selection via Correlation Blockmodel Clustering. – (Proposes an advanced clustering algorithm ensuring intra-cluster correlation structures are homogeneous, useful for diversification and asset selection)[10].* Additional: Portfolio Optimizer Blog – Correlation-Based Clustering: Spectral Clustering Methods (2022) – (Explains spectral clustering algorithms like Blockbuster and SPONGE for financial correlations)[11];Medium article – Hierarchical Clustering: Understanding Market Structures (Co-Variance, 2024) – (Highlights why hierarchical clustering is suited for market data and its advantage over k-means in not requiring a preset number of clusters)[3];Raffinot, T. (2018). Hierarchical Clustering-Based Asset Allocation. Journal of Portfolio Management 44(2) – (Uses hierarchical clustering for building portfolios, notes differences between linkage methods and recommends techniques like Ward/average for balanced clusters)[8].[1] [4] diva-portal.orghttps://www.diva-portal.org/smash/get/diva2:196577/FULLTEXT01.pdf[2] [5] [11] Correlation-Based Clustering: Spectral Clustering Methods | Portfolio Optimizerhttps://portfoliooptimizer.io/blog/correlation-based-clustering-spectral-clustering-methods/[3] 2. Hierarchical Clustering: Understanding Market Structures | by Co-Variance | Mediumhttps://co-variance.medium.com/2-hierarchical-clustering-understanding-market-structures-5b904c8f2a81[6] [7] [8] [9] 12.2 Hierarchical Clustering and Dendrograms | Portfolio Optimizationhttps://bookdown.org/palomar/portfoliooptimizationbook/12.2-hierarchical-clustering-and-dendrograms.html[10] Asset Selection via Correlation Blockmodel Clustering by Wenpin Tang, Xiao Xu, Xun Yu Zhou :: SSRNhttps://papers.ssrn.com/sol3/papers.cfm?abstract_id=3813247